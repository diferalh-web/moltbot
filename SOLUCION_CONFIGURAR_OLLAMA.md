# üîß Soluci√≥n: Error al Configurar Ollama en Open WebUI

## ‚ùå Problema

Est√°s intentando configurar Ollama en **"External Tools" ‚Üí "Manage Tool Servers"**, pero esa secci√≥n es para **servidores OpenAPI compatibles**, no para Ollama como backend de modelos.

## ‚úÖ Soluci√≥n Correcta

### Opci√≥n 1: Buscar la Secci√≥n Correcta de Ollama

En Open WebUI, la configuraci√≥n de Ollama como backend est√° en otro lugar:

1. **Cierra el modal "Edit Connection"** (haz clic en la X)
2. **Busca en el men√∫ lateral izquierdo** una de estas opciones:
   - **"Connections"** o **"Conexiones"**
   - **"Backend"** o **"Backend Configuration"**
   - **"Ollama"** (puede estar como secci√≥n separada)
   - O busca en **"General"** ‚Üí puede haber una subsecci√≥n de Ollama

3. Si encuentras la secci√≥n de Ollama, ah√≠ deber√≠as poder agregar:
   - URL: `http://host.docker.internal:11437` (para Qwen)
   - URL: `http://host.docker.internal:11438` (para Code)

### Opci√≥n 2: Configurar mediante Variables de Entorno

Si no encuentras la secci√≥n correcta en la interfaz, puedo recrear Open WebUI con una configuraci√≥n que detecte autom√°ticamente todos los servicios Ollama.

**Ejecuta este comando:**
```powershell
.\scripts\configurar-multi-ollama-open-webui.ps1
```

Luego, los modelos deber√≠an aparecer autom√°ticamente.

### Opci√≥n 3: Usar Solo Mistral (Soluci√≥n Temporal)

Por ahora, puedes usar solo **Mistral** que ya est√° funcionando. Es un modelo muy vers√°til que puede:
- Chat general
- Programaci√≥n b√°sica
- Preguntas y respuestas

Los dem√°s modelos (Qwen, CodeLlama) los puedes agregar despu√©s cuando encontremos la secci√≥n correcta.

## üîç D√≥nde Buscar la Configuraci√≥n de Ollama

En diferentes versiones de Open WebUI, la configuraci√≥n puede estar en:

1. **Settings ‚Üí Connections** (m√°s com√∫n)
2. **Settings ‚Üí General ‚Üí Backend**
3. **Settings ‚Üí Backend Configuration**
4. **Directamente en el selector de modelos** (algunas versiones permiten escribir URLs)

## üêõ Si No Encuentras la Secci√≥n

**Puedo ayudarte de dos formas:**

1. **Recrear Open WebUI** con una configuraci√≥n que detecte autom√°ticamente todos los servicios Ollama
2. **Verificar la versi√≥n de Open WebUI** y buscar la documentaci√≥n espec√≠fica para esa versi√≥n

## üìã Modelos Disponibles

Aunque no los veas en el selector todav√≠a, estos modelos est√°n disponibles y funcionando:

- ‚úÖ `mistral:latest` (puerto 11436) - **Ya visible**
- ‚è≥ `qwen2.5:7b` (puerto 11437) - Necesita configuraci√≥n
- ‚è≥ `codellama:34b` (puerto 11438) - Necesita configuraci√≥n
- ‚è≥ `deepseek-coder:33b` (puerto 11438) - Necesita configuraci√≥n

## üí° Recomendaci√≥n

**Por ahora, usa Mistral** que ya est√° funcionando. Es un modelo excelente para la mayor√≠a de tareas. Podemos configurar los dem√°s modelos despu√©s.

---

**¬øQuieres que:**
1. **Te ayude a buscar la secci√≥n correcta** en Settings?
2. **Recrear Open WebUI** con configuraci√≥n autom√°tica?
3. **Usar solo Mistral** por ahora?












