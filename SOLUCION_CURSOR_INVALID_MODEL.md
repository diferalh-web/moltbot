# üîß Soluci√≥n: Error "Invalid Model" en Cursor

## Problema
Cursor muestra el error "invalid model" cuando intentas usar Ollama con la URL `http://localhost:11438/v1`.

## ‚úÖ Verificaci√≥n

He verificado que:
- ‚úÖ Ollama responde correctamente en `/v1/models`
- ‚úÖ Los modelos est√°n disponibles: `deepseek-coder:33b` y `codellama:34b`
- ‚úÖ El endpoint `/v1/models` devuelve los modelos en formato correcto

## üéØ Soluciones a Probar

### Soluci√≥n 1: Configuraci√≥n desde API Keys (Recomendada)

En Cursor Settings ‚Üí API Keys:

1. **Override OpenAI Base URL:**
   - URL: `http://localhost:11438/v1` ‚úÖ (ya lo tienes)

2. **OpenAI API Key:**
   - Key: `ollama` (o cualquier texto)

3. **IMPORTANTE: No uses el campo "Model" en API Keys**
   - Deja el campo de modelo vac√≠o o no lo configures ah√≠
   - El modelo se selecciona desde el chat o desde otra secci√≥n

### Soluci√≥n 2: Configurar Modelo desde el Chat

1. Abre el chat (`Ctrl + L`)
2. Busca el selector de modelos (puede estar en la parte superior o en un men√∫)
3. Si hay un campo de texto para escribir el modelo, escribe: `deepseek-coder:33b`
4. O busca en la lista si aparece

### Soluci√≥n 3: Usar el Modelo sin Seleccionarlo

Aunque diga "invalid model", intenta usar el chat directamente:

1. Abre el chat
2. Escribe una pregunta
3. Verifica en los logs de Docker si hay actividad:
   ```powershell
   docker logs ollama-code --tail 20 -f
   ```

Si ves actividad, el modelo est√° funcionando aunque diga "invalid model".

### Soluci√≥n 4: Probar con Codellama

Prueba con el otro modelo disponible:

**En settings.json:**
```json
{
    "cursor.model": "codellama:34b",
    "cursor.modelProvider": "openai",
    "cursor.modelBaseUrl": "http://localhost:11438/v1",
    "cursor.apiKey": "ollama"
}
```

### Soluci√≥n 5: Sin /v1 (API Nativa de Ollama)

Si `/v1` no funciona, prueba sin √©l (aunque esto puede no funcionar con Cursor):

```json
{
    "cursor.model": "deepseek-coder:33b",
    "cursor.modelProvider": "custom",
    "cursor.modelBaseUrl": "http://localhost:11438",
    "cursor.apiKey": "ollama"
}
```

### Soluci√≥n 6: Usar el Proxy de Ollama

Si tienes `ollama-proxy` corriendo en el puerto 11440:

```json
{
    "cursor.model": "deepseek-coder:33b",
    "cursor.modelProvider": "openai",
    "cursor.modelBaseUrl": "http://localhost:11440/v1",
    "cursor.apiKey": "ollama"
}
```

Y en la interfaz de Cursor:
- Override OpenAI Base URL: `http://localhost:11440/v1`

## üîç Verificar qu√© Modelos Ve Cursor

Para ver qu√© modelos est√° detectando Cursor:

1. Abre Cursor Settings
2. Ve a la secci√≥n "Models"
3. Busca si hay alguna lista de modelos disponibles
4. O busca en el chat si hay un dropdown de modelos

## üìù Configuraci√≥n Actual Recomendada

He actualizado tu `settings.json` con esta configuraci√≥n:

```json
{
    "cursor.model": "deepseek-coder:33b",
    "cursor.modelProvider": "openai",
    "cursor.modelBaseUrl": "http://localhost:11438/v1",
    "cursor.apiKey": "ollama",
    "cursor.chat.model": "deepseek-coder:33b",
    "cursor.chat.modelProvider": "openai",
    "cursor.chat.modelBaseUrl": "http://localhost:11438/v1",
    "cursor.chat.apiKey": "ollama"
}
```

## üß™ Probar si Funciona

Aunque diga "invalid model", prueba:

1. **Reinicia Cursor completamente**
2. **Abre el chat** (`Ctrl + L`)
3. **Escribe una pregunta simple:** "Hello, how are you?"
4. **En otra terminal, monitorea los logs:**
   ```powershell
   docker logs ollama-code --tail 20 -f
   ```

Si ves actividad en los logs cuando escribes en el chat, **el modelo est√° funcionando** aunque Cursor muestre el error.

## ‚ö†Ô∏è Limitaci√≥n Conocida

Cursor puede mostrar "invalid model" pero a√∫n as√≠ usar el modelo si:
- La URL base est√° correcta
- El modelo existe en Ollama
- La configuraci√≥n est√° en `settings.json`

El error puede ser solo una validaci√≥n de la interfaz que no afecta el funcionamiento real.

## üéØ Pr√≥ximos Pasos

1. ‚úÖ Reinicia Cursor
2. ‚úÖ Prueba usar el chat directamente (ignora el error si aparece)
3. ‚úÖ Verifica los logs de Docker para confirmar que funciona
4. ‚úÖ Si funciona, el error es solo cosm√©tico

---

**¬øEl modelo funciona aunque diga "invalid model"?** Verifica los logs de Docker para confirmarlo.


