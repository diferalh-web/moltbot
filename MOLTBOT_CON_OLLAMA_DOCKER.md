# üê≥ Conectar Moltbot a Ollama en Docker

## üìã Resumen

Esta gu√≠a te mostrar√° c√≥mo:
1. Ejecutar Ollama en un contenedor Docker
2. Configurar Moltbot para usar Ollama como proveedor de modelos
3. Conectar ambos servicios

## üöÄ Paso 1: Instalar Docker en la VM

**En tu terminal SSH**, ejecuta:

```bash
# Actualizar sistema
sudo apt update

# Instalar Docker
sudo apt install -y docker.io docker-compose

# Agregar usuario al grupo docker (para usar sin sudo)
sudo usermod -aG docker $USER

# Reiniciar sesi√≥n o ejecutar:
newgrp docker

# Verificar instalaci√≥n
docker --version
docker-compose --version
```

## ü¶ô Paso 2: Ejecutar Ollama en Docker

### Opci√≥n A: Docker Run (Simple)

```bash
# Crear directorio para datos de Ollama
mkdir -p ~/ollama-data

# Ejecutar Ollama en Docker
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ~/ollama-data:/root/.ollama \
  --restart unless-stopped \
  ollama/ollama:latest
```

### Opci√≥n B: Docker Compose (Recomendado)

Crea el archivo `~/docker-compose-ollama.yml`:

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ~/ollama-data:/root/.ollama
    restart: unless-stopped
    # Opcional: limitar recursos
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
```

Luego ejecuta:

```bash
docker-compose -f ~/docker-compose-ollama.yml up -d
```

## ‚úÖ Paso 3: Verificar que Ollama Est√° Corriendo

```bash
# Ver estado del contenedor
docker ps | grep ollama

# Ver logs
docker logs ollama

# Probar conexi√≥n
curl http://localhost:11434/api/tags
```

## üì• Paso 4: Descargar Modelos en Ollama

```bash
# Entrar al contenedor
docker exec -it ollama ollama pull llama2
# O
docker exec -it ollama ollama pull mistral
# O
docker exec -it ollama ollama pull codellama
# O cualquier otro modelo que prefieras

# Ver modelos instalados
docker exec -it ollama ollama list
```

## üîß Paso 5: Configurar Moltbot para Usar Ollama

### Opci√≥n A: Configurar mediante CLI

**En tu terminal SSH**, ejecuta:

```bash
cd ~/moltbot

# Configurar modelo de Ollama
pnpm start config set models.default.provider ollama
pnpm start config set models.default.model llama2
pnpm start config set models.default.baseURL http://localhost:11434
```

### Opci√≥n B: Editar Configuraci√≥n Manualmente

```bash
# Editar archivo de configuraci√≥n
nano ~/.openclaw/openclaw.json
```

Agrega o modifica la secci√≥n de modelos:

```json
{
  "models": {
    "default": {
      "provider": "ollama",
      "model": "llama2",
      "baseURL": "http://localhost:11434",
      "apiKey": "ollama"  // Ollama no requiere API key real, pero algunos clientes la piden
    }
  }
}
```

### Opci√≥n C: Usar Variables de Entorno

```bash
# Configurar variables de entorno
export OPENCLAW_MODEL_PROVIDER=ollama
export OPENCLAW_MODEL_NAME=llama2
export OPENCLAW_MODEL_BASE_URL=http://localhost:11434

# Ejecutar Moltbot
pnpm start gateway
```

## üß™ Paso 6: Probar la Conexi√≥n

```bash
# Probar que Ollama responde
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?",
  "stream": false
}'

# Probar con Moltbot
cd ~/moltbot
pnpm start agent --message "Hola, ¬øc√≥mo est√°s?" --local
```

## üîó Paso 7: Configurar Red Docker (Si es Necesario)

Si Moltbot est√° en la VM y Ollama en Docker, ambos deber√≠an poder comunicarse en `localhost:11434`.

Si necesitas acceso desde fuera de la VM:

```bash
# Modificar docker-compose para exponer en todas las interfaces
# Cambiar en docker-compose-ollama.yml:
#   - "11434:11434"
#   a:
#   - "0.0.0.0:11434:11434"
```

## üìù Configuraci√≥n Avanzada

### M√∫ltiples Modelos

Puedes configurar m√∫ltiples modelos en Moltbot:

```bash
pnpm start config set models.llama2.provider ollama
pnpm start config set models.llama2.model llama2
pnpm start config set models.llama2.baseURL http://localhost:11434

pnpm start config set models.mistral.provider ollama
pnpm start config set models.mistral.model mistral
pnpm start config set models.mistral.baseURL http://localhost:11434
```

### Usar Modelo Espec√≠fico

```bash
# Especificar modelo al ejecutar
pnpm start agent --model llama2 --message "Hola"
```

## üÜò Soluci√≥n de Problemas

### Ollama no responde
```bash
# Verificar que est√° corriendo
docker ps | grep ollama

# Ver logs
docker logs ollama

# Reiniciar
docker restart ollama
```

### Moltbot no puede conectar a Ollama
```bash
# Verificar que Ollama est√° accesible
curl http://localhost:11434/api/tags

# Verificar configuraci√≥n de Moltbot
pnpm start config get models
```

### Modelo no encontrado
```bash
# Ver modelos disponibles en Ollama
docker exec -it ollama ollama list

# Descargar el modelo que necesitas
docker exec -it ollama ollama pull <nombre-del-modelo>
```

## üìö Modelos Recomendados para Ollama

- **llama2** - Modelo general bueno
- **mistral** - R√°pido y eficiente
- **codellama** - Especializado en c√≥digo
- **llama3** - √öltima versi√≥n (si est√° disponible)
- **phi** - Modelo peque√±o y r√°pido

---

**Empieza ejecutando Ollama en Docker y luego configura Moltbot para usarlo.**












